{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Install PyTorch Lightning**\n",
    "\n",
    "> **Pip users**: pip install lightning\n",
    ">\n",
    "> **Conda users**: conda install lightning -c conda-forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Define a LightningModule**\n",
    "\n",
    "A LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning as L\n",
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "\n",
    "        # returns a new tensor with the same data as the self tensor but of a different shape.\n",
    "        x = x.view(x.size(0), -1) # equivalent to nn.Flatten()\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "\n",
    "        # logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "autoencoder = LitAutoEncoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Define a dataset**\n",
    "\n",
    "Lightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup data\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)  # utils (from torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Train the model**\n",
    "\n",
    "The Lightning Trainer “mixes” any LightningModule with any dataset and abstracts away all the engineering complexity needed for scale.\n",
    "\n",
    "The Lightning Trainer automates 40+ tricks including:\n",
    "\n",
    "* Epoch and batch iteration\n",
    "\n",
    "* `optimizer.step()`, loss.backward(), optimizer.zero_grad() calls\n",
    "\n",
    "* Calling of `model.eval()`, enabling/disabling grads during evaluation\n",
    "\n",
    "* Checkpoint Saving and Loading\n",
    "\n",
    "* Tensorboard (see loggers options)\n",
    "\n",
    "* Multi-GPU support\n",
    "\n",
    "* TPU\n",
    "\n",
    "* 16-bit precision AMP support\n",
    "\n",
    ">\n",
    "> **Trainer**: https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    ">\n",
    "> **Trainer tricks**: https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Adombi\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: c:\\Users\\Adombi\\Documents\\AvadDocuments\\003-MyProjects\\001-Poste2Prof\\008-Organisation\\001-AlgoPyTorch\\002-Lightening\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n",
      "c:\\Users\\Adombi\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6948022c9fd840e1b7864a117216f519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = L.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Use the model**\n",
    "\n",
    "Once you’ve trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[ 0.3383, -0.0650,  0.3435],\n",
      "        [ 0.2860, -0.0427,  0.4032],\n",
      "        [ 0.2403,  0.0127,  0.3409],\n",
      "        [ 0.1840, -0.0436,  0.3422]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\n",
    "autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# choose your trained nn.Module\n",
    "encoder = autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Visualize training**\n",
    "\n",
    "> Install tensorboard\n",
    "> * Open CMD and `pip install tensorboard`\n",
    ">\n",
    "> Install torch-tb-profiler\n",
    "> * Open CMD and `pip install torch-tb-profiler`\n",
    ">\n",
    "> Run tensorboard\n",
    ">\n",
    "> * ``\n",
    "\n",
    "If you have tensorboard installed, you can use it for visualizing experiments. Run this on your commandline and open your browser to http://localhost:6006/\n",
    "\n",
    "\n",
    "**Use tensorboard with PyTorch**\n",
    "\n",
    "> https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Supercharge training**\n",
    "\n",
    "Enable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on 4 GPUs\n",
    "trainer = L.Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    " )\n",
    "\n",
    "# train 1TB+ parameter models with Deepspeed/fsdp\n",
    "trainer = L.Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    "    strategy=\"deepspeed_stage_2\",\n",
    "    precision=16\n",
    " )\n",
    "\n",
    "# 20+ helpful flags for rapid idea iteration\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=1\n",
    " )\n",
    "\n",
    "# access the latest state of the art techniques\n",
    "trainer = L.Trainer(callbacks=[StochasticWeightAveraging(...)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Maximize flexibility**\n",
    "\n",
    "Lightning’s core guiding principle is to always provide maximal flexibility without ever hiding any of the PyTorch.\n",
    "\n",
    "Lightning offers 5 added degrees of flexibility depending on your project’s complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.1. Customize training loop**\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#lightning-hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def backward(self, loss):\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.2. Extend the Trainer**\n",
    "\n",
    "If you have multiple lines of code with similar functionalities, you can use callbacks to easily group them together and toggle all of those lines on or off at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(callbacks=[AWSCheckpoints()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8.3. Use a raw PyTorch loop**\n",
    "\n",
    "For certain types of work at the bleeding-edge of research, Lightning offers experts full control of optimization or the training loop in various ways.\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/model/build_model_advanced.html#manual-optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
